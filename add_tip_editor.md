Integrating an AI Editor Agent into the Live On Tip Pipeline: Analysis and Implementation Plan
1. Executive Summary
This report outlines a strategic plan to enhance the liveon application's content pipeline. The user's request is to integrate an AI-powered "Editor Agent" into the existing TipPipeline. The current pipeline (Aggregator -> Generator -> Publisher) lacks a critical quality assurance step, leading to the potential publication of tips that may be uninteresting, repetitive, or of low quality.   

The proposed solution introduces a new TipEditorAgent and a "Generate-Review-Refine" loop. This agent will act as an automated quality gate, programmatically evaluating each generated tip against predefined criteria (e.g., conciseness, "interestingness," actionability, novelty). Tips that fail this review will be sent back to the TipGenerator with specific feedback for regeneration, up to a configurable limit.

This enhancement will significantly improve the quality and consistency of user-facing content, reduce manual oversight, and provide a scalable framework for content generation. The implementation will primarily involve modifying the TipPipeline orchestration service , creating a new TipEditorAgent class, and updating the TipGenerator  to accept feedback for iterative refinement.   

2. Analysis of the Current Tip Generation Pipeline
A review of the provided application source code reveals two distinct content pipelines: one for articles  and one for tips. The article pipeline already employs a more sophisticated, multi-step process involving a SummarizerAgent  which creates a draft, and a separate EditorAgent  which refines and polishes that draft.   

The tip pipeline, by contrast, is a simpler, linear process as defined in app/scripts/run_tip_pipeline.py  and app/services/pipeline.py :   

Initiation: The run_tip_pipeline.py script  is executed. It initializes the LongevityNewsAggregator, TipGenerator, and TipPublisher.   

Aggregation: LongevityNewsAggregator  fetches content from various FeedSources.   

Generation: The entire batch of aggregated content is passed to the TipGenerator. This agent uses a single LLM call to produce one TipDraft.   

Publication: This TipDraft is immediately passed to the TipPublisher. The publisher performs a basic duplicate check based on the title and tags against the LocalSQLiteContentRepository  before saving the new Tip  to the database.   

Identified Gap:

The core deficiency in this workflow is the absence of any qualitative review between the generation step  and the publication step. The TipGenerator acts as a single point of failure for content quality. If it produces a tip that is factually questionable, grammatically poor, or simply uninteresting (e.g., a generic "remember to drink water" message), the TipPublisher has no mechanism to reject it. The TipPublisher's current logic  only checks for existing tips with an identical title or tag set, it does not evaluate the quality or novelty of the content itself.   

This contrasts sharply with the application's more mature article ContentPipeline , which explicitly separates the initial drafting stage (SummarizerAgent) from the refinement and quality control stage (EditorAgent). The user's request is to implement a similar, more robust model for the TipPipeline, introducing a "human-like" review and refinement loop using an AI agent.   

3. Proposed Architecture: The "Editor-in-the-Loop" Model
To address the identified gap, this plan proposes the introduction of a new agent, the TipEditorAgent, and a modification of the TipPipeline orchestrator to create a "Generate-Review-Refine" loop. This new workflow will be: Generate -> Review -> (Regenerate if needed) -> Publish.

3.1. New Component: TipEditorAgent
A new service class, TipEditorAgent, will be created, likely in a new file named app/services/tip_editor.py. This class will be responsible for acting as an automated quality assurance gate.

Function: The agent will receive a TipDraft object  generated by the TipGenerator. It will also receive a list of existing Tip objects , fetched from the LocalSQLiteContentRepository , to check for novelty and repetitiveness.   

Action: It will use an LLM to evaluate the draft against a predefined rubric. This rubric will enforce the user's requirements:

Short: Is the content concise and suitable for a "tip" format?

Interesting: Does it provide a novel insight, a specific detail, or a unique perspective? Is it more valuable than common knowledge?

High-Quality: Is the information accurate, grammatically correct, and written in the appropriate brand voice (encouraging, trustworthy)?

Output: The agent will return a new data object, TipReviewResult, which will explicitly state whether the tip is is_approved (a boolean) and provide structured feedback (a string) explaining its decision. This feedback is critical for the "refine" step.

3.2. New Data Model: TipReviewResult
To facilitate clear communication between the TipEditorAgent and the TipPipeline orchestrator, a new data model will be defined, likely in app/models/tip_editor.py.

Python
# Proposed: app/models/tip_editor.py
from __future__ import annotations
from dataclasses import dataclass, field
from app.models.tip import TipDraft  # Based on 

@dataclass(slots=True)
class TipReviewResult:
    """
    Structured output from the TipEditorAgent after reviewing a TipDraft.
    """
    is_approved: bool
    feedback: str | None = None
    revised_draft: TipDraft | None = None # Allows the editor to make minor tweaks
3.3. Modified Orchestration: TipPipeline.run()
The primary logic change will occur in app/services/pipeline.py. The TipPipeline class will be modified to accept both the TipEditorAgent and the LocalSQLiteContentRepository (for its lookup capabilities) during initialization. The run method will be rewritten to manage the new generative loop.   

Proposed TipPipeline Structure (modified app/services/pipeline.py):

Python
# In app/services/pipeline.py

#... (Protocol definitions for SupportsAggregation, SupportsTipGeneration, SupportsTipPublishing)...
from app.models.tip_editor import TipReviewResult # New import

class SupportsTipEditing(Protocol):
    """Protocol for the new TipEditorAgent."""
    def review(self, draft: TipDraft, existing_tips: Sequence) -> TipReviewResult:
       ...

class SupportsTipLookup(Protocol):
    """Protocol for the repository to fetch existing tips."""
    def get_latest_tips(self, *, limit: int = 5) -> list:...
    def find_article_by_source_url(self, url: str) -> Article | None:... # From 

@dataclass(slots=True)
class TipPipelineResult:
    """Structured summary of a tip pipeline execution."""
    #... (existing fields)...
    generation_attempts: int = 1
    editor_feedback: list[str] = field(default_factory=list)
    
    #... (existing properties)...

@dataclass(slots=True)
class TipPipeline:
    """Coordinate the workflow from aggregation through tip publication."""

    aggregator: SupportsAggregation
    generator: SupportsTipGeneration
    editor: SupportsTipEditing         # NEW
    publisher: SupportsTipPublishing
    repository: SupportsTipLookup   # NEW

    MAX_GENERATION_ATTEMPTS: int = 3 # Configurable retry limit

    def run(
        self,
        *,
        limit_per_feed: int = 5,
        published_at: datetime | None = None,
    ) -> TipPipelineResult:
        """Execute the tip pipeline with an AI-powered review loop."""

        aggregation = self.aggregator.gather(limit_per_feed=limit_per_feed)
        warnings = list(aggregation.errors)
        errors: list[str] =

        if not aggregation.items:
            warnings.append("No aggregated content available to generate tips.")
            return TipPipelineResult(
                aggregation=aggregation, 
                warnings=warnings, 
                errors=errors,
                generation_attempts=0
            )

        # Fetch recent tips for the editor to check against
        try:
            existing_tips = self.repository.get_latest_tips(limit=20)
        except Exception as e:
            warnings.append(f"Could not fetch existing tips for comparison: {e}")
            existing_tips =

        feedback: str | None = None
        draft: TipDraft | None = None
        review_result: TipReviewResult | None = None
        attempt = 0
        final_feedback_log: list[str] =

        while attempt < self.MAX_GENERATION_ATTEMPTS:
            attempt += 1
            try:
                # Pass aggregation items and optional feedback (from previous loop)
                draft = self.generator.generate(aggregation.items, feedback=feedback)
            except Exception as exc:
                error_msg = f"Tip generator failed on attempt {attempt}: {exc}"
                errors.append(error_msg)
                # If generator fails, we can't proceed with this attempt
                feedback = f"Generation failed with error: {exc}. Please try again, focusing on robustness."
                final_feedback_log.append(error_msg)
                continue # Try again

            if not draft or not draft.body.strip():
                warning_msg = f"Generator produced empty draft on attempt {attempt}."
                warnings.append(warning_msg)
                feedback = "The generated tip was empty. Please generate a valid tip with a title and body."
                final_feedback_log.append(feedback)
                continue

            try:
                # The new editor agent reviews the draft
                review_result = self.editor.review(draft, existing_tips)
            except Exception as exc:
                error_msg = f"Tip editor failed on attempt {attempt}: {exc}"
                errors.append(error_msg)
                # If editor fails, we can't trust the draft. Force regeneration.
                feedback = "The previous tip could not be reviewed due to an error. Please try generating a different tip on the same topic."
                final_feedback_log.append(error_msg)
                review_result = None # Ensure we don't proceed with a failed review
                continue

            if review_result.is_approved:
                # Successfully generated and approved, exit the loop
                break
            
            # Not approved, prepare for next loop
            feedback = review_result.feedback or "The tip was rejected for unspecified reasons. Please try a different approach."
            warnings.append(f"Tip draft rejected (Attempt {attempt}/{self.MAX_GENERATION_ATTEMPTS}): {feedback}")
            final_feedback_log.append(f"Attempt {attempt} rejected: {feedback}")
            review_result = None # Clear result as it was not approved

        # Check loop exit condition
        if review_result is None or not review_result.is_approved:
            errors.append(f"Failed to generate an approved tip after {self.MAX_GENERATION_ATTEMPTS} attempts.")
            return TipPipelineResult(
                aggregation=aggregation,
                draft=draft, # Last failed draft
                errors=errors,
                warnings=warnings,
                generation_attempts=attempt,
                editor_feedback=final_feedback_log,
            )

        # We have an approved tip. Use the editor's revision if provided, otherwise use the generator's draft.
        final_draft = review_result.revised_draft or draft
        
        if final_draft is None:
             # This should be unreachable due to the check above, but as a safeguard:
             errors.append("Pipeline logic error: Approved result has no draft.")
             return TipPipelineResult(
                aggregation=aggregation,
                errors=errors,
                warnings=warnings,
                generation_attempts=attempt,
                editor_feedback=final_feedback_log,
             )

        try:
            publication = self.publisher.publish(final_draft, published_at=published_at)
        except Exception as exc:
            errors.append(f"Tip publisher failed: {exc}")
            return TipPipelineResult(
                aggregation=aggregation,
                draft=final_draft,
                errors=errors,
                warnings=warnings,
                generation_attempts=attempt,
                editor_feedback=final_feedback_log,
            )

        # Success
        tip = publication.tip
        if not publication.created:
            warnings.append("Tip already exists; skipped creating a duplicate.")

        return TipPipelineResult(
            aggregation=aggregation,
            draft=final_draft,
            tip=tip,
            publication=publication,
            errors=errors,
            warnings=warnings,
            generation_attempts=attempt,
            editor_feedback=final_feedback_log,
        )

#... (ContentPipeline class remains unchanged)...
3.4. Architectural Data Flow Comparison
This new architecture fundamentally changes the data flow from a simple pipe to a process with a decision gate and a feedback loop, enhancing robustness.

Current Flow: run_tip_pipeline.py -> TipPipeline.run() -> Aggregator.gather() -> TipGenerator.generate() -> TipPublisher.publish() -> SQLiteRepo.save_tip()

Proposed Flow: run_tip_pipeline.py -> TipPipeline.run()

TipPipeline calls Aggregator.gather()

TipPipeline calls Repository.get_latest_tips()

Loop (starts):

TipPipeline calls TipGenerator.generate(items, feedback?) -> returns TipDraft

TipPipeline calls TipEditorAgent.review(draft, existing_tips) -> returns TipReviewResult

Decision Gate:

If is_approved == True: Loop ends.

If is_approved == False: Loop repeats from step 4, passing TipReviewResult.feedback to the generator.

TipPipeline calls TipPublisher.publish(approved_draft) -> SQLiteRepo.save_tip()

TipPipeline returns TipPipelineResult

This "agent-based" model, where one agent (Generator) creates and another (Editor) critiques, is a powerful pattern. The TipPipeline acts as the meta-agent or orchestrator. By logging the feedback messages from the TipReviewResult (which will be added to TipPipelineResult), a valuable dataset can be built for future fine-tuning of the TipGenerator, progressively teaching it to avoid common mistakes.

4. Implementation Guide & Task Breakdown
To implement this, the following tasks should be executed in order.

Task 1: Create app/models/tip_editor.py

Create this new file to house the TipReviewResult data class.

Define the TipReviewResult dataclass as specified in section 3.2.

This class will import TipDraft from app.models.tip.   

Task 2: Create app/services/tip_editor.py

Create the new TipEditorAgent class. This file will be analogous to app/services/editor.py.   

It should import TipDraft , TipReviewResult, SupportsInvoke , and ChatPromptTemplate.   

It must have an __init__(self, llm: SupportsInvoke) method.

It must implement the review(self, draft: TipDraft, existing_tips: Sequence) method. This method must:

Format the existing_tips into a simple string list of titles.

Format the TipDraft into a JSON string.

Call the LLM with the prompt (specified in Section 5).

Use robust JSON parsing logic (similar to _parse_payload in SummarizerAgent ) to handle potential malformed LLM outputs (e.g., markdown code fences).   

Instantiate and return a TipReviewResult object based on the parsed JSON.

Task 3: Modify app/services/tip_generator.py    

Modify generate method signature:

Change from: def generate(self, items: Sequence[AggregatedContent]) -> TipDraft:

To: def generate(self, items: Sequence[AggregatedContent], feedback: str | None = None) -> TipDraft:

Modify _default_prompt factory:

The TIP_HUMAN_PROMPT constant must be updated to conditionally include the feedback, as detailed in Section 5. This will require using Jinja2 templating syntax (e.g., {% if feedback %}).

Modify generate method logic:

Update the variables passed to self.prompt.format_messages(...) to include the new feedback variable.

The call will now look like:

Python
messages = self.prompt.format_messages(
    notes=notes_block,
    sources=sources_block,
    current_date=datetime.now(timezone.utc).date().isoformat(),
    feedback=feedback  # Pass the feedback string
)
Task 4: Modify app/services/pipeline.py    

Add the SupportsTipEditing and SupportsTipLookup protocols to the file (see code in section 3.3).

Update the TipPipelineResult dataclass to add the new logging fields:

generation_attempts: int = 1

editor_feedback: list[str] = field(default_factory=list)

Update the TipPipeline dataclass attributes to include:

editor: SupportsTipEditing

repository: SupportsTipLookup

Replace the entire existing TipPipeline.run method with the new implementation from section 3.3, which includes the retry loop logic.

Task 5: Modify app/scripts/run_tip_pipeline.py    

Modify _build_pipeline function:

The LocalSQLiteContentRepository is already initialized. This instance must be passed to the TipPipeline constructor.

The llm variable created by _create_tip_llm should be used to instantiate the new TipEditorAgent.

The final TipPipeline instantiation must be updated.

Add from app.services.tip_editor import TipEditorAgent to the imports.

Change the instantiation from:

Python
#...
generator = TipGenerator(llm=llm)
repository = LocalSQLiteContentRepository()
publisher = TipPublisher(repository)
return TipPipeline(aggregator=aggregator, generator=generator, publisher=publisher)
To:

Python
#...
generator = TipGenerator(llm=llm)
repository = LocalSQLiteContentRepository()
publisher = TipPublisher(repository)
editor = TipEditorAgent(llm=llm) # <-- New

return TipPipeline(
    aggregator=aggregator,
    generator=generator,
    editor=editor,           # <-- New
    publisher=publisher,
    repository=repository    # <-- New
)
Modify main function (for logging):

Update the final payload dictionary to include the new logging fields from TipPipelineResult:

Python
payload = {
    "aggregation": asdict(result.aggregation),
    "draft": asdict(result.draft) if result.draft else None,
    "tip": asdict(result.tip) if result.tip else None,
    "publication": asdict(result.publication) if result.publication else None,
    "warnings": result.warnings,
    "errors": result.errors,
    "succeeded": result.succeeded,
    "created": result.created,
    "generation_attempts": getattr(result, 'generation_attempts', 1), # Add new metrics
    "editor_feedback": getattr(result, 'editor_feedback',)      # Add new metrics
}
5. Detailed Prompts for LLM Implementation
These prompts are designed for maximum clarity and enforce the required JSON output structure, which is critical for programmatic parsing.

Prompt 1: Tip Editor Agent (for new file app/services/tip_editor.py)
This prompt instructs the LLM to act as a critical editor, check against a clear rubric, and return a structured JSON object.

Python
# This should be defined as a constant in app/services/tip_editor.py
# then instantiated using ChatPromptTemplate.from_messages()

TIP_EDITOR_SYSTEM_PROMPT = """You are an exacting, helpful, and concise senior editor for a health and wellness publication. Your sole purpose is to act as a quality-control gate for AI-generated content. You must be strict.

Your review will be based on these 5 criteria:
1.  **Concise:** The tip (title + body) should be short and easily digestible. Under 60 words is ideal.
2.  **Interesting / Novel:** The tip must not be generic (e.g., "sleep more," "drink water," "exercise"). It should provide a specific insight or actionable advice that the user may not have known.
3.  **Actionable:** The user must be able to act on the tip.
4.  **High-Quality:** The text must be grammatically perfect, clear, and written in an encouraging, professional tone.
5.  **Non-Repetitive:** The tip must be sufficiently different from recently published tips.

You MUST respond in a specific JSON format and nothing else. Your entire response must be only the JSON object.
"""

TIP_EDITOR_HUMAN_PROMPT = """
Please review the provided below.

Compare it against the to check for repetition.


{% if existing_tips %}
{% for tip in existing_tips %}
- {{ tip.title }}
{% endfor %}
{% else %}
- None
{% endif %}


{{ draft_json }}

Based on the 5 criteria (Concise, Interesting, Actionable, High-Quality, Non-Repetitive), evaluate the draft.

If the draft is excellent and meets all criteria, set "is_approved" to true. You may optionally provide minor copyedits in "revised_draft" to improve it further.

If the draft fails *any* criterion, set "is_approved" to false. In the "feedback" field, provide a single, clear, constructive sentence for the writer, explaining *why* it was rejected and *how* to fix it.

**Respond with ONLY the raw JSON object.**

Example of a GOOD response (for approval):
{
  "is_approved": true,
  "feedback": "Clear and actionable.",
  "revised_draft": {
    "title": "A Slightly Better Title",
    "body": "The original body text, but with a small typo fixed.",
    "tags": ["nutrition", "fasting"]
  }
}

Example of a GOOD response (for rejection):
{
  "is_approved": false,
  "feedback": "This tip is too generic. Please provide a more specific, actionable insight related to the source material.",
  "revised_draft": null
}

Your response:
"""
Prompt 2: Tip Generator (Modified app/services/tip_generator.py)
This modified prompt  introduces a conditional block to handle feedback for regeneration. This requires replacing the existing TIP_HUMAN_PROMPT in app/services/tip_generator.py with this new version.   

Python
# This will replace the existing TIP_HUMAN_PROMPT in app/services/tip_generator.py

TIP_HUMAN_PROMPT = """
{% if feedback %}


A previous tip draft was rejected by our editor. Please generate a new tip based on the original and below, but this time, you MUST address the.


{{ feedback }}

{% else %}


Using the research notes below, create a single actionable longevity tip suitable for our app.
Focus on clear takeaways people can apply today.
{% endif %}


{notes}


{sources}


{current_date}


You MUST return *only* a valid JSON object with the exact following structure:
{{
  "title": "short tip title",
  "body": "1-2 paragraph Markdown explanation with optional bullet list",
  "tags": ["keywords"],
  "metadata": {{
    "sources": ["https://..."],
    "confidence": "low|medium|high"
  }}
}}
"""
6. Verification and Deployment Strategy
Unit & Integration Testing:

New: TestTipEditorAgent: A new test file (app/tests/test_tip_editor.py) should be created.

Test 1: Provide a good draft. Assert review_result.is_approved is True.

Test 2: Provide a generic draft (e.g., "Drink water"). Assert is_approved is False and feedback is populated.

Test 3: Provide a draft that is too long. Assert is_approved is False and feedback mentions length.

Test 4: Provide a draft with a title matching an existing_tips entry. Assert is_approved is False and feedback mentions repetition.

Modify: TestTipGenerator : Add a new test case.   

Test: Call generator.generate() with a non-null feedback string. Assert the resulting LLM prompt (captured by the DummyLLM.calls attribute) contains the "REVISE AND REGENERATE" block and the feedback text.

Modify: TestTipPipeline :   

Update the test setup to inject a mock TipEditorAgent and a mock LocalSQLiteContentRepository (for get_latest_tips).

Test 1 (Happy Path): Mock the editor to approve on the first try. Assert result.succeeded is True and result.created is True.

Test 2 (Retry Path): Mock the editor's review method to return is_approved=False on the first call and is_approved=True on the second. Assert result.succeeded is True, result.generation_attempts == 2, and the generator was called twice.

Test 3 (Failure Path): Mock the editor to reject 3 times (or MAX_GENERATION_ATTEMPTS). Assert result.succeeded is False and an "Failed to generate" error is present in result.errors.

Deployment:

The existing Dockerfile  and deployment manifests  do not require modification. The changes are contained within the Python application logic and do not add new system dependencies.   

The change can be rolled out as a standard update to the longevity-coach-deployment by building and pushing the new image tag and applying the updated deployment.yaml (if the image tag is changed). The existing deploy.ps1 script  will handle this process.   

Monitoring:

After deployment, logs should be monitored for TIP_PIPELINE_ERROR and TIP_PIPELINE_WARNING.

Specifically, tracking the new warnings related to "Tip draft rejected" will be crucial. If this warning appears frequently, it indicates either the TipGenerator's base prompt  needs improvement or the TipEditorAgent's criteria (in its new prompt) are too strict. This creates an actionable feedback loop for future prompt engineering.   

